---
title: "Week 3: AR/ MA/ ARMA/ ARIMA"
format:
  revealjs:
    slide-number: true
    show-slide-number: all 
jupyter: python3
---

# Recap: Stationarity

```{python}
#| fig-column: margin
#| echo: true
import numpy
import matplotlib.pyplot as plt

mean = 0
std = 1 
num_samples = 100
samples = numpy.random.normal(mean, std, size=num_samples)
plt.plot(samples)
plt.show()
```

## ACF 

```{python}
import pandas as pd
from matplotlib import pyplot as plt
from statsmodels.graphics.tsaplots import plot_acf
plot_acf(samples, lags=20)
plt.show()
```

White noise implies stationarity. Stationarity does not imply white noise.

## Non-Stationary Time Series

**1. Deterministic trend**

$$Y_t  = f(t) + \epsilon_t$$


where $\epsilon_t \sim iid(0, \sigma^2)$, $t = 1, 2, ...T$

Mean of the process is time dependent, but the variance of the process is constant.

A trend is deterministic if it is a nonrandom function of time.

## Non-Stationary Time Series (cont.)

**2. Random walk** 

$$Y_t = Y_{t-1} + \epsilon_t$$

- Random walk has a stochastic trend.

- Model behind naive method.

A trend is said to be stochastic if it is a random function of time.



## Non-Stationary Time Series (cont.)

**3. Random walk with drift**

$$Y_t = \alpha+  Y_{t-1} + \epsilon_t$$

- Random walk with drift has a stochastic trend and a deterministic trend.

- Model behind drift method.


## Random walk


$$
\begin{aligned}
  Y_t &= Y_{t-1} + \epsilon_t \\
     Y_1    &= Y_0 + \epsilon_1 \\
         Y_2 &=  Y_1 + \epsilon_2=Y_0 + \epsilon_1 + \epsilon_2\\
          Y_3 &=  Y_2 + \epsilon_3=Y_0 + \epsilon_1 + \epsilon_2 +\epsilon_3\\
          .   \\
          Y_t &=Y_{t-1} + \epsilon_t=Y_0 + \epsilon_1 + \epsilon_2 + \epsilon_3 +...+ \epsilon_t = Y_0 + \sum_{i=1}^{t} \epsilon_t
\end{aligned}
$$

Mean: $E(Y_t) = Y_0$.

Variance: $Var(Y_t)=t \sigma^2$.

## Random walk with drift


$$
\begin{aligned}
  Y_t &= Y_{t-1} + \epsilon_t \\
     Y_1    &= \alpha+Y_0 + \epsilon_1 \\
         Y_2 &= \alpha+ Y_1 + \epsilon_2=2 \alpha+Y_0 + \epsilon_1 + \epsilon_2\\
          Y_3 &= \alpha+ Y_2 + \epsilon_3= 3 \alpha+ Y_0 + \epsilon_1 + \epsilon_2 +\epsilon_3\\
          .   \\
          Y_t &= \alpha+Y_{t-1} + \epsilon_t= t \alpha+ Y_0 + \epsilon_1 + \epsilon_2 + \epsilon_3 +...+ \epsilon_t \\
          Y_t &= t \alpha + Y_0 + \sum_{i=1}^{t} \epsilon_t
\end{aligned}
$$

## Random walk with drift (cont.)


It has a *deterministic trend* $(Y_0 + t \alpha)$ and a *stochastic trend* $\sum_{i=1}^{t} \epsilon_t$.

Mean: $E(Y_t) = Y_0 + t\alpha$

Variance: $Var(Y_t) = t\sigma^2$.

There is a trend in both mean and variance. 


## Common trend removal (de-trending) procedures

1. Deterministic trend: Time-trend regression

      The trend can be removed by fitting a deterministic polynomial time trend. The residual series after removing the trend will give us the de-trended series.

1. Stochastic trend: Differencing
 
      The process is also known as a **Difference-stationary process**.
      
# Notation: I(d)

Integrated to order $d$: Series can be made stationary by differencing $d$ times.
 
 - Known as $I(d)$ process.
 

**Question: ** Show that random walk process is an $I(1)$ process.

The random walk process is called a unit root process.
(If one of the roots turns out to be one, then the process is called unit root process.)

## Random walk

```{python}
#| echo: true
import numpy as np
rw = np.cumsum(samples)
plt.plot(rw)
plt.show()
```

## Random walk - ACF

```{python}
plot_acf(rw, lags=20)
plt.show()
```

## Difference series

```{python}
df = pd.DataFrame(rw, columns = ['Values'])
df['Lag 1'] = df['Values'].diff()
df['Lag 2'] = df['Values'].diff().diff()
df
```

## Plot Lag 1 series

```{python}
plt.plot(df['Values'].diff())
plt.show()
```

## ACF Lag 1 series

```{python}
diff = df['Lag 1']
plot_acf(diff.dropna(), lags=20)
plt.show()
```


## Example 2

```{python}
#| echo: true
#| eval: false
import numpy as np, pandas as pd
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
import matplotlib.pyplot as plt
plt.rcParams.update({'figure.figsize':(9,7), 'figure.dpi':120})

# Import data
df = pd.read_csv('wwwusage.csv', names=['value'], header=0)

# Original Series
fig, axes = plt.subplots(2, 2, sharex=True)
axes[0, 0].plot(df.value); axes[0, 0].set_title('Original Series')
plot_acf(df.value, ax=axes[0, 1], lags=np.arange(len(df)))

# 1st Differencing
axes[1, 0].plot(df.value.diff()); axes[1, 0].set_title('1st Order Differencing')
plot_acf(df.value.diff().dropna(), ax=axes[1, 1], lags=np.arange(len(df) - 1))
plt.show()

```


##

```{python}

import numpy as np, pandas as pd
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
import matplotlib.pyplot as plt
plt.rcParams.update({'figure.figsize':(9,7), 'figure.dpi':120})

# Import data
df = pd.read_csv('wwwusage.csv', names=['value'], header=0)

# Original Series
fig, axes = plt.subplots(2, 2, sharex=True)
axes[0, 0].plot(df.value); axes[0, 0].set_title('Original Series')
plot_acf(df.value, ax=axes[0, 1], lags=np.arange(len(df)))

# 1st Differencing
axes[1, 0].plot(df.value.diff()); axes[1, 0].set_title('1st Order Differencing')
plot_acf(df.value.diff().dropna(), ax=axes[1, 1], lags=np.arange(len(df) - 1))
plt.show()

```

## 2nd order differencing

```{python}
#| echo: true
plot_acf(df.value.diff().diff().dropna())
plt.show()
```

## Variance stabilization

Eg:

- Square root: $W_t = \sqrt{Y_t}$

- Logarithm: $W_t = log({Y_t})$

     - This very useful.
     
     - Interpretable: Changes in a log value are **relative (percent) changes on the original sclae**.
     
## Monthly Airline Passenger Numbers 1949-1960

```{python}
#| echo: true
airpassenger = pd.read_csv('AirPassengers.csv')
from datetime import datetime
import plotnine
from plotnine import *
airpassenger['Month']= pd.to_datetime(airpassenger['Month'])
ggplot(airpassenger, aes(x='Month', y='#Passengers'))+geom_line()
```

## Monthly Airline Passenger Numbers 1949-1960 - log

```{python}
#| echo: true
import numpy as np
airpassenger['naturallog'] = np.log(airpassenger['#Passengers']) 
ggplot(airpassenger, aes(x='Month', y='naturallog'))+geom_line()
```

## Box-Cox transformation

$$
  w_t=\begin{cases}
    log(y_t), & \text{if $\lambda=0$} \newline
    (Y_t^\lambda - 1)/ \lambda, & \text{otherwise}.
  \end{cases}
$$


Different values of $\lambda$ gives you different transformations.

- $\lambda=1$: No **substantive** transformation

- $\lambda = \frac{1}{2}$: Square root plus linear transformation

- $\lambda=0$: Natural logarithm

- $\lambda = -1$: Inverse plus 1

Balance the seasonal fluctuations and random variation across the series.

## Box-Cox transformation

```{python}
#| echo: true
# import modules
import numpy as np
from scipy import stats
 
y2,fitted_lambda = stats.boxcox(airpassenger['#Passengers'])

```


##  Box-Cox transformation: Exploring the output

```{python}
#| echo: true

fitted_lambda
```

```{python}
#| echo: true
y2

```




## ARMA(p, q) model


$$Y_t=c+\phi_1Y_{t-1}+...+\phi_p Y_{t-p}+ \theta_1\epsilon_{t-1}+...+\theta_q\epsilon_{t-q}+\epsilon_t$$

- These are stationary models.

- They are only suitable for **stationary series**.

## ARIMA(p, d, q) model

Differencing --> ARMA

**Step 1: Differencing**

$$Y'_t = (1-B)^dY_t$$

**Step 2: ARMA**

$$Y'_t=c+\phi_1Y'_{t-1}+...+\phi_p Y'_{t-p}+ \theta_1\epsilon_{t-1}+...+\theta_q\epsilon_{t-q}+\epsilon_t$$

# Step 1: Plot data

1. Detect unusual observations in the data

1. Detect non-stationarity by visual inspections of plots

Stationary series:

- has a constant mean value and fluctuates around the mean.

- constant variance.

- no pattern predictable in the long-term.

##

```{python}
from sktime import *
from sktime.datasets import load_airline
from sktime.utils.plotting import plot_series
y = load_airline()
plot_series(y)
```

## Step 2: Split time series into training and test

Specify the forecast horizon

```{python}
#| echo: true
import numpy as np
import pandas as pd
from sktime.forecasting.base import ForecastingHorizon
fh = ForecastingHorizon(
    pd.PeriodIndex(pd.date_range("1960-01", periods=12, freq="M")), is_relative=False
)
fh

```

## Plot training and test series

```{python}
from sktime.forecasting.model_selection import temporal_train_test_split
y_train, y_test = temporal_train_test_split(y, fh=fh)
plot_series(y_train, y_test, labels=["y_train", "y_test"])
```

##

1. Need transformations?

2. Need differencing?

## Step 3: Apply transformations

```{python}
#| echo: true
import numpy as np
y_train.naturallog = np.log(y_train) 
plot_series(y_train.naturallog)
```

## Step 4: Take difference series

**Identifying non-stationarity by looking at plots**
  
- Time series plot

- The ACF of stationary data drops to zero relatively quickly.

- The ACF of non-stationary data decreases slowly.

- For non-stationary data, the value of $r_1$ is often large and positive.

## Non-seasonal differencing and seasonal differencing

**Non seasonal first-order differencing:** $Y'_t=Y_t - Y_{t-1}$

<!--Miss one observation-->

**Non seasonal second-order differencing:** $Y''_t=Y'_t - Y'_{t-1}$

<!--Miss two observations-->

**Seasonal differencing:** $Y_t - Y_{t-m}$

<!--To get rid from prominent seasonal components. -->

- For monthly, $m=12$, for quarterly, $m=4$.

<!--We will loosefirst 12 observations-->


- Seasonally differenced series will have $T-m$ observations.
<!--Usually we do not consider differencing more than twice. -->

> There are times differencing once is not enough. However, in practice,it is almost never necessary to go beyond second-order differencing.
<!--Even the second-order differencing is very rare.-->

## ACF of log-transformation series

```{python}
#| echo: true
plot_acf(y_train.naturallog, lags=50)
```

## Take seasonal difference series

```{python}
#| echo: true
y_train.naturallog.diff12 = y_train.naturallog.diff(12)
y_train.naturallog.diff12
```

## Take seasonal difference series (cont.)

```{python}
#| echo: true
y_train.naturallog.diff12.head(20)
```



## ACF - diff(log(data), 12)

```{python}
#| echo: true
plot_acf(y_train.naturallog.diff12.dropna(), lags=50)
plt.show()
```

## ACF - First differencing on diff(log(data), 12)

```{python}
#| echo: true
y_train.naturallog.diff12.diff = y_train.naturallog.diff12.diff()
plot_acf(y_train.naturallog.diff12.diff.dropna(), lags=50)
plt.show()
```

## PACF - First differencing on diff(log(data), 12)


```{python}
#| echo: true
plot_pacf(y_train.naturallog.diff12.diff.dropna(), lags=50)
plt.show()
```

# Step 5: Examine the ACF/PACF to identify a suitable model

## AR(p)

- ACF dies out in an exponential or damped
sine-wave manner.

- there is a significant spike at lag $p$ in PACF, but
none beyond $p$.

## MA(q)

- ACF has all zero spikes beyond the $q^{th}$ spike.

- PACF dies out in an exponential or damped
sine-wave manner.

## Seasonal components

- The seasonal part of an AR or MA model will be seen
in the seasonal lags of the PACF and ACF.


## ARIMA(0,0,0)(0,0,1)12 will show
 
  - a spike at lag 12 in the ACF but no other significant spikes.

  - The PACF will show exponential decay in the seasonal lags  12, 24, 36, . . . .
  
## ARIMA(0,0,0)(1,0,0)12 will show

  - exponential decay in the seasonal lags of the ACF.
    
  - a single significant spike at lag 12 in the PACF.


## Step 5: Examine the ACF/PACF to identify a suitable model (cont.)

- $d=1$ and $D=1$ (from step 4)

- Significant spike at lag 1 in ACF suggests
non-seasonal MA(1) component.

- Significant spike at lag 12 in ACF suggests seasonal
MA(1) component.

- Initial candidate model: $ARIMA(0,1,1)(0,1,1)_{12}$.

- By analogous logic applied to the PACF, we could also have started with $ARIMA(1,1,0)(1,1,0)_{12}$.
  
## Models   
  
**Initial model:**

$ARIMA(0,1,1)(0,1,1)_{12}$

$ARIMA(1,1,0)(1,1,0)_{12}$

**Try some variations of the initial model:**

$ARIMA(0,1,1)(1,1,1)_{12}$

$ARIMA(1,1,1)(1,1,0)_{12}$

$ARIMA(1,1,1)(1,1,1)_{12}$

##

**Try some variations**

Both the ACF and PACF show significant spikes at lag 3, and almost significant spikes at lag 3, indicating that some additional non-seasonal terms need to be included in the model.

$ARIMA(3,1,1)(1,1,1)_{12}$

$ARIMA(1,1,3)(1,1,1)_{12}$

$ARIMA(3,1,3)(1,1,1)_{12}$

## Fitting ARIMA models

```{python}
#| echo: true
from sktime.forecasting.arima import ARIMA
forecaster1 = ARIMA(  
    order=(0, 1, 1),
    seasonal_order=(0, 1, 1, 12),
    suppress_warnings=True)
forecaster1.fit(y_train.naturallog)    
```


## Step 6: Check residual series

```{python}
#| echo: true
fhtrain = ForecastingHorizon(
    pd.PeriodIndex(pd.period_range(start='1949-01', end='1959-12', freq='M')), is_relative=False
)
fhtrain

```

## Obtain predictions for the training period

```{python}
#| echo: true
y_pred_train = forecaster1.predict(fhtrain)
y_pred_train

```

## Obtain residual series

```{python}
#| echo: true
residual = y_train.naturallog - y_pred_train
residual
```

## Plot residuals

```{python}
#| echo: true
plot_series(residual)
```

## Plot residuals (cont.)

```{python}
#| echo: true
plot_acf(residual.dropna(), lags=50)
```

## Plot residuals (cont.)

```{python}
import matplotlib.pyplot as plt
import numpy as np
plt.hist(residual)
plt.show()
```

Your turn: remove the outlier and draw the histogram

## Step 7: Generate forecasts


```{python}
#| echo: true
y_pred_1 = forecaster1.predict(fh)
y_pred_1

```

## Back transformation

```{python}
#| echo: true
y_pred_1.exp = np.exp(y_pred_1)
y_pred_1.exp
```

## Plot training, test, and forecasts

```{python}
#| echo: true
plot_series(y_train, y_test, y_pred_1.exp, labels=["y_train", "y_test", "y_forecast"])

```

## Evaluation

```{python}
#| echo: true
from sktime.performance_metrics.forecasting import \
    mean_absolute_percentage_error
mean_absolute_percentage_error(y_test, y_pred_1.exp, symmetric=False)
```

## Your Turn

Fit other variants of ARIMA models and identify the best ARIMA model for the series.