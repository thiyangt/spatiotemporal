---
title: "Week 2: Time Series Forecasting"
format:
  revealjs:
    slide-number: true
    show-slide-number: all 
jupyter: python3
---

## sktime: Unified Python library for time series machine learning

Installing sktime from PyPI

To install sktime with core dependencies, excluding soft dependencies, via pip type:

```
pip install sktime
```

To install sktime with maximum dependencies, including soft dependencies, install with the all_extras modifier:
```
pip install sktime[all_extras]
```
Extracted from:

https://www.sktime.org/en/stable/installation.html

## Probabilistic time series model 

Let ${X_1, X_2, ...}$ be a sequence of random variables. Then the joint distribution of the random vector $([X_1, X_2, ..., X_n])$ is 

$$P[X_1 \le x_1, X_2 \le x_2, ..., X_n \le x_n]$$
 

where $-\infty < x_1, ..., x_n < \infty$ and $(n = 1, 2, ...)$


## Mean function


The **mean function** of ${X_t}$ is

$$\mu_X(t)=E(X_t).$$

## Covariance function

The **covariance function** of ${X_t}$ is

$$\gamma_X(r, s)=Cov(X_r, X_s)=E[(X_r-\mu_X(r))(X_s-\mu_X(s))]$$

for all integers $(r)$ and $(s)$.

The covariance function of ${X_t}$ at lag $(h)$ is defined by
$$\gamma_X(h):=\gamma_X(h, 0)=\gamma(t+h, t)=Cov(X_{t+h}, X_t).$$


## Autocovariance function

The auto covariance function of ${X_t}$ at lag $(h)$ is

$$\gamma_X(h)=Cov(X_{t+h}, X_t).$$
**Autocorrelation function**

The autocorrelation function of ${X_t}$ at lag $(h)$ is

$$\rho_X(h)=\frac{\gamma_X(h)}{\gamma_X(0)}=Cor(X_{t+h}, X_t).$$

## Weekly stationary

A time series ${X_t}$ is called weekly stationary if 

- $\mu_X(t)$ is independent of $t$.

- $\gamma_X(t+h, t)$` is independent of $(t)$ for each $(h)$.

In other words the statistical properties of the time series (mean, variance, autocorrelation, etc.) do not depend on the time at which the series is observed, that is no trend or seasonality. However,  a time series with cyclic behaviour (but with no trend or seasonality) is stationary.

## Strict stationarity of a time series

A time series ${X_t\}$ is called weekly stationary if the random vector $[X_1, X_2..., X_n]$ and $[X_{1+h}, X_{2+h}..., X_{n+h}]$ have the same joint distribution for all integers $(h)$ and $(n > 0)$.

# Simple time series models

## 1. iid noise

1. no trend or seasonal component

2. observations are independent and identically distributed (iid) random variables with zero mean. 

3. Notation: ${X_t} \sim IID(0, \sigma^2)$

4. plays an important role as a building block for more complicated time series.

## 

```{python}
#| echo: true
import numpy
import matplotlib.pyplot as plt

mean = 0
std = 1 
num_samples = 1000
samples = numpy.random.normal(mean, std, size=num_samples)
samples

```

##

```{python}
plt.plot(samples)
plt.show()
```

## 2. White noise

If ${X_t}$ is a sequence of uncorrelated random variables, each with zero mean and variance $\sigma^2$, then such a sequence is referred to as **white noise**.

Note: Every $(IID(0, \sigma^2)$ sequence is $(WN(0, \sigma^2)$ but not conversely.

## 3. Random walk

A random walk process is obtained by cumulatively summing iid random variables. If ${S_t, t=0, 1, 2, ...}$ is a random walk process, then
$S_0 =0$

$S_1=0+X_1$

$S_2=0+X_1+X_2$

$...$

$S_t=X_1+X_2+...+X_t.$

## Question

Is ${S_t, t=0, 1, 2, ...}$ a weak stationary process?

## Identifying non-stationarity in the mean

- Using time series plot

- ACF plot

   - ACF of stationary time series will drop to relatively quickly.
   
   - The ACF of non-stationary series decreases slowly.
   
   - For non-stationary series, the ACF at lag 1 is often large and positive.
   
# Elimination of Trend and Seasonality by Differencing

- Differencing helps to stabilize the mean.

## Backshift notation: 

$$BX_t=X_{t-1}$$

## Ordinary differencing

The first-order differencing can be defined as

$$\nabla X_t = X_t-X_{t-1}=X_t-BX_t=(1-B)X_t$$
where $\nabla=1-B$.

The second-order differencing

$$\nabla^2X_t=\nabla(\nabla X_t)=\nabla(X_t-X_{t-1})=\nabla X_t - \nabla X_{t-1}$$

$$\nabla X_t - \nabla X_{t-1}=(X_t-X_{t-1})-(X_{t-1}-X_{t-2})$$
- In practice, we seldom need to go beyond second order differencing. 

## Seasonal differencing

- differencing between an observation and the corresponding observation from the previous year.

$$\nabla_mX_t=X_t-X_{t-m}=(1-B^m)X_t$$
where $(m)$ is the number of seasons. For monthly, $(m=12)$, for quarterly $(m=4)$.

For monthly series

$$\nabla_{12}X_t=X_t-X_{t-12}$$
##

Twice-differenced series

$$\nabla^2_{12}X_t=\nabla_{12}X_t-\nabla_{12}X_{t-1}$$
$$\nabla_{12}X_t-\nabla_{12}X_{t-1}=(X_t-X_{t-12})-(X_{t-1}-X_{t-13}$$
If seasonality is strong, the seasonal differencing should be done first.

# Linear filter model

A linear filter is an operation $L$ which transform the white noise process into another time series ${X_t}$.

White noise $(\epsilon_t)$ $\to$ Linear Filter $(\psi(B))$ $\to$ Output $(X_t)$

# Autoregressive models

current value  = linear combination of past values + current error 

An autoregressive model of order $p$, $AR(P)$ model can be written as

$$x_t= c+ \phi_1x_{t-1}+\phi_2x_{t-2}+...+\phi_px_{t-p}+\epsilon_t,$$
where $\epsilon_t$ is white noise.  

- Similar to multiple linear regression model but with lagged values of $(x_t)$ as predictors.

## Question 

Show that $(AR(P)$ is a linear filter with transfer function $\phi^{-1}(B)$, where $(\phi(B)=1-\phi_1B-\phi_2B^2-...-\phi_pB^p.)$

## Stationary condition for AR(P)

The roots of $(\phi(B)=0)$ (characteristic equation) must lie outside the unit circle.


## Stationarity Conditions for AR(1)

Let's consider $(AR(1))$ process,

$$x_t=\phi_1x_{t-1}+\epsilon_t.$$

Then, 

$$(1-\phi_1 B)x_t=\epsilon_t.$$

This may be written as,
$$x_t=(1-\phi_1 B)^{-1}\epsilon_t=\sum_{j=0}^{\infty}\phi_1^j\epsilon_{t-j}.$$

##

Hence,

$$\psi(B)=(1-\phi_1 B)^{-1}=\sum_{j=0}^{\infty}\phi_1^jB^j$$

If $(\phi_1| &lt; 1,)$ the $(AR(1))$ process is stationary.

This is equivalent to saying the roots of $(1-\phi_1B=0)$ must lie outside the unit circle.

## Geometric series

$$a+ar+ar^2+ar^3+...=\frac{a}{1-r}$$

for $(|r|&lt;1.)$

## AR(1) process

$$x_t=c+\phi_1x_{t-1}+\epsilon_t,$$

- when $\phi_1=0$ - equivalent to white noise process

- when $\phi_1=0$ and $c=0$ - random walk

- when $\phi_1=1$ and $c \neq 0$ - random walk with drift

- when $\phi_1 < 0$ - oscillates around the mean

## 

slides: https://tsforecasting-thiyanga.netlify.app/slides/timeseries2.html#27