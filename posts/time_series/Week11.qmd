---
title: "Week 11: Spatial Kriging/ Spatio-temporal Kriging"
format:
  revealjs:
    slide-number: true
    show-slide-number: all 
jupyter: python3
---

```{python}
#| fig-column: margin
#| echo: false
import numpy
import matplotlib.pyplot as plt

```



## Random process

- The value of a property at any place $x$, is denoted by $z(x)$. The $z(x)$ is one of an infinity of values of a
random variable $Z(x)$ at that place. We call it a ‘realization’ of the
process.



- The set of random values at all such places, again infinite in number,
in a region is a random process, and also denoted $Z(x)$.



- The random variable is spatially correlated at some scale.

## Stationarity

The notion of stationarity underpins geostatistics and allows us to
assume that there is the same degree of variation from place to place.

We can represent the random process by the model

$$Z(x) = \mu + \epsilon(x)$$

where $\mu$ is the mean of the process and $\epsilon(x)$ is a random quantity with a
mean of zero and a covariance, $C(\textbf{h})$, given by

$$C(\textbf{h}) = E[\epsilon(x)\epsilon(x+\textbf{h})]$$



which is equivalent to

##

$$
\begin{aligned}
C(h) &= E[(Z(x)-\mu)(Z(x+h)-\mu)] \\
&= E[Z(x)Z(x+h)-\mu^2]
\end{aligned}
$$

In these equations $h$ is the separation between samples in both distance
and direction; $Z(x)$ and $Z(x + h)$ are the values of $Z$ at places $x$
and $x + h$.

##

Under a weaker assumption of intrinsic stationarity in which the
expected differences are zero, i.e. $E[Z(x) − Z(x + h)] = 0$.

Then,

$$
\begin{aligned}
\gamma(\textbf{h}) &= \frac{1}{2}var[z(x)-z(x+h)]\\ &= \frac{1}{2}E[\{Z(x) - Z(x+\textbf{h})\}^2]
\end{aligned}
$$

## Covariance function and correlogram

$$\gamma(\textbf{h}) = C(0) - C(\textbf{h})$$

Correlogram

$$\rho(\textbf{h})= \frac{C(\textbf{h})}{\sigma^2}$$

## Computing and modelling variograms

1. Plot the experimental variogram.

2. Choose several models that appear to have the right shape and fit
each in turn by weighted least squares (Cressie, 1985; McBratney
andWebster, 1986) in an accredited program.

3. Plot the fitted models on the graph of the experimental variogram
and assess whether the fit looks reasonable.

4. If all plausible models seem to fit well, choose the one with the
smallest residual sum of squares (RSS) or smallest mean square.



##

5. One might be able to improve a fit in the above sense by elaborating
themodel. Any combination of the simple valid models is itself valid. The Akaike information criterion,
the AIC (Akaike, 1973), may help to answer. The aim is to minimize it (Webster and McBratney,
1989; Webster and Oliver, 2007).

## Factors affect the reliability of the experimental variogram

- Size of sample

- Lag interval and bin width

- Marginal distribution of the data

- Anisotropy

- Trend

## cross-validation provides a means of choosing among plausiblemodels for variograms


- Each
and every one of the $N$ data points is omitted in turn fromthe set of data
and its value there is predicted by ordinary punctual kriging with the
proposed model. 

- Three statistics are then calculated; they are the
mean error (ME), themean squared error (MSE) and themean squared
deviation ratio (MSDR)



## Example: Data

- `meuse` dataset contains measurements for concentrations of different elements, over an area in the Netherlands.

##

```{python}
#| fig-column: margin
#| echo: true
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import plotnine
import skgstat as skg
from pprint import pprint
from plotnine import ggplot, geom_point, aes, stat_smooth, facet_wrap
import warnings
warnings.filterwarnings("ignore")
```


## Load data


```{python}
#| fig-column: margin
#| echo: true
src = skg.data.meuse()
print(src.get('origin'))

coords, vals = src.get('sample')
# make a nice table
df = pd.DataFrame({'x': coords[:, 0], 'y': coords[:, 1], 'lead': vals.flatten()})

```

##

```{python}
#| fig-column: margin
#| echo: true
df.head()

```

##

Plotting the x and y coordinates and visually inspect how the z spread out.

```{python}
#| fig-column: margin
#| echo: true
fig, ax = plt.subplots(1, 1, figsize=(9, 9))
art = ax.scatter(coords[:, 0], coords[:, 1], s=50, c=vals.flatten(), cmap='plasma')
plt.colorbar(art)


```

#

```{python}
#| fig-column: margin
#| echo: true

(ggplot(df, aes('x', 'y', color='lead'))
 + geom_point())

```


# Variogram modelling

- How to choose an appropriate model function

- How to judge fitting quality

- Sample size influence

Source: https://scikit-gstat.readthedocs.io/en/latest/auto_examples/tutorial_03_variogram_models.html#sphx-glr-auto-examples-tutorial-03-variogram-models-py


# Kriging

$$Z(x) = \mu + \epsilon(x)$$

**Ordinary kriging:** $\mu$ is an unkown constant (prediction based on coordinates)

**Simple kriging:** $\mu$ is a known constant

**Universal kriging:** $\mu(s)$ is some deterministic function - prediction based on coordinates and
additional regressors (elevation)



# Variogram modelling and Kriging


https://scikit-gstat.readthedocs.io/en/latest/auto_examples/tutorial_01_getting_started.html


## Spatio-temporal Modelling

Monthly averages for some environmental parameter in Germany (Gräler, 2012)

![](figsp.png)

## Spatio-temporal approaches

- **Slice-wise:** The easiest adoption is to do interpolation per
slice fitting a variogram model for each time
slice

- **Pooled:** The variogram is fitted based on all
spatio-temporal data and is used to predict
each time slice separately with the same model

**Evolving** models mix the both extremes such that the
variogram model adopts to the daily situation
(e.g. in terms of overall variability, the sill) but
range and the nugget/sill ratio depend on
larger data samples.

## Spatio-temporal variogram

Let spatio-temporal random field be the $Z(s, t)$. Then,

$$\gamma(h, u)=E[Z(s, t) - Z(s+h, t+u)]^2$$

At any location $(s, t)$ and empirical version

$$\hat{\gamma}(h, u)=\frac{1}{2|N_{h, u}|}\sum_{{(i, j)} \in N_{h, u}}(Z(s_i, t_i)-Z(s_j, t_j))^2$$

## Spatio-temporal with Machine Learning Models

- Feature selection and feature extraction

- Use of dimension reduction techniques

- Deep neural networks

- Machine learning algorithms

- Meta-learning methods (learning-to-learn)

## Problem with Machine Learning models

- ignore the spatial locations of the observations and hence overlook any spatial autocorrelation in the data not accounted for by the covariates.

## Create geographical covariates

"Everything is related to everything else, but near things are more related than distant things."

"The key to making RF applicable to spatial statistics problems, therefore, lies also in preparing geographical (spatial) measures of proximity and connectivity between observations, so that spatial autocorrelation can be accounted for. "

Source: https://soilmapper.org/soilmapping-using-mla.html#a-generic-framework-for-spatial-prediction-using-random-forest

## Geographical covariates

- Geographical coordinates  (i.e., easting and northing).

- Euclidean distances to reference points in the study area. For example, distance to the center and edges of the study area, etc (T Behrens et al. 2018).



# Resources

SciKit-GStat 1.0: a SciPy-flavored geostatistical variogram
estimation toolbox written in Python

Link: https://gmd.copernicus.org/articles/15/2505/2022/gmd-15-2505-2022.pdf


## Acknowledgement

Oliver, M. A., & Webster, R. (2014). A tutorial guide to geostatistics: Computing and modelling variograms and kriging. Catena, 113, 56-69.