{
  "hash": "066aff7fee16e1b675285a3fd8ad4b75",
  "result": {
    "markdown": "---\ntitle: 'Week 8b: Time Series Clustering'\nformat:\n  revealjs:\n    slide-number: true\n    show-slide-number: all\n---\n\n## Is the following series dissimilar or similar?\n\n\n![](dtw1.png)\n\n\n## What is time series clustering?\n\n\n- Group **similar** time series\n\n- What we meant by **similar**?\n\n    - similar profiles\n    \n    - similar features\n    \n    - same best forecasting models\n    \n    - similar data generating process\n    \n- Before clustering think what is the purpose of grouping?\n\n##\n\n![](tclust.png)\nSource: Montero, P., & Vilar, J. A. (2015). TSclust: An R package for time series clustering. Journal of Statistical Software, 62, 1-43.\n\n\n## Time Series Clustering\n\n1. Model-free approaches\n\n2. Model-based approaches\n\n3. Complexity-based approaches\n\n4. Prediction-based approaches\n\n# Model-free approaches\n\n$$\\textbf{X}_T = (X_1, X_2, ..., X_T)^T$$\n\n$$\\textbf{Y}_T = (Y_1, Y_2, ..., Y_T)^T$$\n\n## Minkowski distance\n\n$$d_{Lq}(\\textbf{X}_T, \\textbf{Y}_T) = [\\sum_{t=1}^T(X_t, Y_t)^q]^{(\\frac{1}{q})}$$\n\nWhen $q=1$ - Manhattan distance\n\nWhen $q=2$  - Euclidean distance\n\n- Observations are treated as independent. ($d_{Lq}$ is invariant to permutation over time)\n\n- Very sensitive to time scaling \n\n## Frechet distance\n\nSource: https://pure.tue.nl/ws/portalfiles/portal/93882810/Thesis_Tom_van_Diggelen.pdf\n\n![](fr.png)\n\n##\n![](fr.png)\nSource: https://pure.tue.nl/ws/portalfiles/portal/93882810/Thesis_Tom_van_Diggelen.pdf\n\n## Frechet distance\n\nThe standard Fréchet distance is the minimum leash length required for the person to walk the dog without backtracking. \n\n- Account the ordering of the observations\n\n- Can be computed on series of different lengths\n\n## Dynamic time warping distance (DTW)\n\n[https://www.youtube.com/watch?v=ERKDHZyZDwA](https://www.youtube.com/watch?v=ERKDHZyZDwA){preview-link=\"true\"}\n\n\n\n![](dtw2.png)\n\n## Correlation-based distance\n\nPearson's correlation between $\\textbf{X_T}$ and $\\textbf{Y_T}$: $Cor(\\textbf{X}_T, \\textbf{Y}_T)$\n\n\n\n$$d_{cor1}= \\sqrt{2(1-Cor(\\textbf{X}_T, \\textbf{Y}_T))}$$\n\n$$d_{cor2}= \\left (\\left (\\frac{1-Cor(\\textbf{X}_T, \\textbf{Y}_T)}{1+Cor(\\textbf{X}_T, \\textbf{Y}_T)} \\right )^{\\beta} \\right )^{\\frac{1}{2}}$$\n\nwhere $\\beta \\geq 0$.\n\n## Autocorrelation-based distance\n\nDistance is based on estimated autocorrelation function.\n\nCase 1: Uniform weight\n\n$$d(\\textbf{X}_T, \\textbf{Y}_T) = \\sqrt{\\sum_{i=1}^L(\\hat{\\rho}_{i, X}-\\hat{\\rho}_{i, Y})^2}$$\n\nCase 2: Geometric weights decaying with autocorrelation lag\n\n$$d(\\textbf{X}_T, \\textbf{Y}_T) = \\sqrt{\\sum_{i=1}^Lw(1-w)^i(\\hat{\\rho}_{i, X}-\\hat{\\rho}_{i, Y})^2}$$\n\n\n## Periodogram-based distance\n\nA periodogram is used to identify the dominant periods (or frequencies) of a time series. \n\n- Euclidean distance between the periodogram ordinates (Gives more weight to the shape of the curve)\n\n-  Euclidean distance between the normalized periodogram ordinates (Consider the scale)\n\nReading: https://online.stat.psu.edu/stat510/lesson/6/6.1\n\n## Other model-free measures\n\n- Dissimilarity measures based on nonparametric spectral estimators\n\n- Dissimilarity measure based on wavelet transformation\n\n- Dissimilarity measure based on the symbolicrepresentation SAX\n\n\n# Model-based approaches\n\n## Piccolo distance\n\n- Invertible ARIMA processes\n\n- Euclidean distance between the $AR(\\infty)$\n\nIn-class explanations.\n\n## Invertible condition\n\n**Backshift notation: **\n\n$$BX_t=X_{t-1}$$\nARMA(p, q)\n\n$$x_t= c+ \\phi_1x_{t-1}+\\phi_2x_{t-2}+...+\\phi_px_{t-p} + \\theta_1\\epsilon_{t-1}+\\theta_2\\epsilon _{t-2}+...+\\theta_q \\epsilon_{t-q}+\\epsilon_t,$$\n\n$$x_t= c+ \\phi_1Bx_{t}+\\phi_2B^2x_{t}+...+\\phi_pB^px_{t} + \\theta_1B\\epsilon_{t}+\\theta_2B^2\\epsilon _{t}+...+\\theta_q B^q\\epsilon_{t}+\\epsilon_t,$$\n$$x_t - \\phi_1Bx_{t}-\\phi_2B^2x_{t}-...-\\phi_pB^px_{t} = c + \\theta_1B\\epsilon_{t}+\\theta_2B^2\\epsilon _{t}+...+\\theta_q B^q\\epsilon_{t}+\\epsilon_t,$$\n\n$$\\Phi(B)x_t = c+\\Theta(B)\\epsilon_t$$\n\n\n##\n\n$$\\Phi(B)x_t = c+\\Theta(B)\\epsilon_t$$\n\n$$\\Phi(B) = 1-\\phi_1B-\\phi_2B^2-...-\\phi_pB^p$$\n\n$$\\Theta(B)= 1+\\theta_1B+\\theta_2B^2 _{t}+...+\\theta_q B^q$$\n\nThis process is called invertible if the modulus of all the roots of $\\Theta(B)=0$ are greater than one.\n\n## Demo\n\nhttps://github.com/thiyangt/AR-infinite-coefficients/blob/master/ARinfinite-code.md\n\n## Maharaj distance\n\n- For the class of invertible and stationary ARMA processes\n\n- Based on hypotheses testing to determine whether or not two time series have significantly different generating processes \n\n    Two dissimilarity measures: one is based on test statistics, other one is based on the associated p-value\n    \n# Prediction-based approaches\n\nTwo time series are similar if their forecasts at s specific future time are close.\n\n# Feature-based time series clustering\n\nLink to slides: https://thiyangt.github.io/whyR2021keynote/#1\n\n\n**tsfeatures package in Python:** https://pypi.org/project/tsfeatures/\n\n#\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\nimport matplotlib.pyplot as plt\n```\n:::\n\n\n## Data\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ndf = pd.read_csv(\"AirPassengers.csv\", index_col=0)\ndf.index = pd.to_datetime(df.index)\ny = df[\"#Passengers\"]\ny.name = \"n_passengers\"\n\ny.plot(title=\"Airline passengers\");\n```\n\n::: {.cell-output .cell-output-display}\n![](week8b_files/figure-revealjs/cell-3-output-1.png){width=798 height=449}\n:::\n:::\n\n\n##\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nseasonal_decomp = seasonal_decompose(y, model=\"additive\")\nseasonal_decomp.plot();\n```\n\n::: {.cell-output .cell-output-display}\n![](week8b_files/figure-revealjs/cell-4-output-1.png){width=950 height=471}\n:::\n:::\n\n\n##\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nseasonal_decomp = seasonal_decompose(y, model=\"multiplicative\")\nseasonal_decomp.plot();\n```\n\n::: {.cell-output .cell-output-display}\n![](week8b_files/figure-revealjs/cell-5-output-1.png){width=950 height=471}\n:::\n:::\n\n\n#\n\n![](admul.png)\n\nAdditive and multiplicative seasonality – can you identify them correctly? https://kourentzes.com/forecasting/2014/11/09/additive-and-multiplicative-seasonality/\n\n## Time series features\n\nTalagala, T. S., Hyndman, R. J., & Athanasopoulos, G. (2018). Meta-learning how to forecast time series. Monash Econometrics and Business Statistics Working Papers, 6(18), 16.\n\nhttps://www.monash.edu/business/ebs/research/publications/ebs/wp06-2018.pdf\n\n# Complexity-based approaches\n\n- Compression-based dissimilarity measures\n\n- Permutation distribution clustering\n\n- Complexity-invariant dissimilarity measure\n\n",
    "supporting": [
      "week8b_files"
    ],
    "filters": [],
    "includes": {}
  }
}