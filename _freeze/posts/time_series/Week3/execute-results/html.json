{
  "hash": "09cb61435e69e66e38500a5fa6838db7",
  "result": {
    "markdown": "---\ntitle: 'Week 3: AR/ MA/ ARMA/ ARIMA'\nformat:\n  revealjs:\n    slide-number: true\n    show-slide-number: all\n---\n\n# Recap: Stationarity\n\n::: {.cell .fig-column-margin execution_count=1}\n``` {.python .cell-code}\nimport numpy\nimport matplotlib.pyplot as plt\n\nmean = 0\nstd = 1 \nnum_samples = 100\nsamples = numpy.random.normal(mean, std, size=num_samples)\nplt.plot(samples)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Week3_files/figure-revealjs/cell-2-output-1.png){width=792 height=411}\n:::\n:::\n\n\n## ACF \n\n::: {.cell execution_count=2}\n\n::: {.cell-output .cell-output-display}\n![](Week3_files/figure-revealjs/cell-3-output-1.png){width=813 height=431}\n:::\n:::\n\n\nWhite noise implies stationarity. Stationarity does not imply white noise.\n\n## Non-Stationary Time Series\n\n**1. Deterministic trend**\n\n$$Y_t  = f(t) + \\epsilon_t$$\n\n\nwhere $\\epsilon_t \\sim iid(0, \\sigma^2)$, $t = 1, 2, ...T$\n\nMean of the process is time dependent, but the variance of the process is constant.\n\n## Non-Stationary Time Series (cont.)\n\n**2. Random walk** \n\n$$Y_t = Y_{t-1} + \\epsilon_t$$\n\n- Random walk has a stochastic trend.\n\n- Model behind naive method.\n\n## Non-Stationary Time Series (cont.)\n\n**3. Random walk with drift**\n\n$$Y_t = \\alpha+  Y_{t-1} + \\epsilon_t$$\n\n- Random walk with drift has a stochastic trend and a deterministic trend.\n\n- Model behind drift method.\n\n\n## Random walk\n\n\n$$\n\\begin{aligned}\n  Y_t &= Y_{t-1} + \\epsilon_t \\\\\n     Y_1    &= Y_0 + \\epsilon_1 \\\\\n         Y_2 &=  Y_1 + \\epsilon_2=Y_0 + \\epsilon_1 + \\epsilon_2\\\\\n          Y_3 &=  Y_2 + \\epsilon_3=Y_0 + \\epsilon_1 + \\epsilon_2 +\\epsilon_3\\\\\n          .   \\\\\n          Y_t &=Y_{t-1} + \\epsilon_t=Y_0 + \\epsilon_1 + \\epsilon_2 + \\epsilon_3 +...+ \\epsilon_t = Y_0 + \\sum_{i=1}^{t} \\epsilon_t\n\\end{aligned}\n$$\n\nMean: $E(Y_t) = Y_0$.\n\nVariance: $Var(Y_t)=t \\sigma^2$.\n\n## Random walk with drift\n\n\n$$\n\\begin{aligned}\n  Y_t &= Y_{t-1} + \\epsilon_t \\\\\n     Y_1    &= \\alpha+Y_0 + \\epsilon_1 \\\\\n         Y_2 &= \\alpha+ Y_1 + \\epsilon_2=2 \\alpha+Y_0 + \\epsilon_1 + \\epsilon_2\\\\\n          Y_3 &= \\alpha+ Y_2 + \\epsilon_3= 3 \\alpha+ Y_0 + \\epsilon_1 + \\epsilon_2 +\\epsilon_3\\\\\n          .   \\\\\n          Y_t &= \\alpha+Y_{t-1} + \\epsilon_t= t \\alpha+ Y_0 + \\epsilon_1 + \\epsilon_2 + \\epsilon_3 +...+ \\epsilon_t \\\\\n          Y_t &= t \\alpha + Y_0 + \\sum_{i=1}^{t} \\epsilon_t\n\\end{aligned}\n$$\n\n## Random walk with drift (cont.)\n\n\nIt has a *deterministic trend* $(Y_0 + t \\alpha)$ and a *stochastic trend* $\\sum_{i=1}^{t} \\epsilon_t$.\n\nMean: $E(Y_t) = Y_0 + t\\alpha$\n\nVariance: $Var(Y_t) = t\\sigma^2$.\n\nThere is a trend in both mean and variance. \n\n\n## Common trend removal (de-trending) procedures\n\n1. Deterministic trend: Time-trend regression\n\n      The trend can be removed by fitting a deterministic polynomial time trend. The residual series after removing the trend will give us the de-trended series.\n\n1. Stochastic trend: Differencing\n \n      The process is also known as a **Difference-stationary process**.\n      \n# Notation: I(d)\n\nIntegrated to order $d$: Series can be made stationary by differencing $d$ times.\n \n - Known as $I(d)$ process.\n \n\n**Question: ** Show that random walk process is an $I(1)$ process.\n\nThe random walk process is called a unit root process.\n(If one of the roots turns out to be one, then the process is called unit root process.)\n\n## Random walk\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nimport numpy as np\nrw = np.cumsum(samples)\nplt.plot(rw)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Week3_files/figure-revealjs/cell-4-output-1.png){width=792 height=411}\n:::\n:::\n\n\n## Random walk - ACF\n\n::: {.cell execution_count=4}\n\n::: {.cell-output .cell-output-display}\n![](Week3_files/figure-revealjs/cell-5-output-1.png){width=813 height=431}\n:::\n:::\n\n\n## Difference series\n\n::: {.cell execution_count=5}\n\n::: {.cell-output .cell-output-display execution_count=33}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Values</th>\n      <th>Lag 1</th>\n      <th>Lag 2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.316111</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.574021</td>\n      <td>-0.257911</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.147831</td>\n      <td>0.426191</td>\n      <td>0.684101</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.290161</td>\n      <td>0.437992</td>\n      <td>0.011801</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.206828</td>\n      <td>-0.496989</td>\n      <td>-0.934980</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>-5.666834</td>\n      <td>-2.046852</td>\n      <td>-2.416976</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>-5.044906</td>\n      <td>0.621928</td>\n      <td>2.668780</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>-5.115827</td>\n      <td>-0.070921</td>\n      <td>-0.692849</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>-4.039724</td>\n      <td>1.076103</td>\n      <td>1.147024</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>-4.663474</td>\n      <td>-0.623750</td>\n      <td>-1.699853</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows Ã— 3 columns</p>\n</div>\n```\n:::\n:::\n\n\n## Plot Lag 1 series\n\n::: {.cell execution_count=6}\n\n::: {.cell-output .cell-output-display}\n![](Week3_files/figure-revealjs/cell-7-output-1.png){width=792 height=411}\n:::\n:::\n\n\n## ACF Lag 1 series\n\n::: {.cell execution_count=7}\n\n::: {.cell-output .cell-output-display}\n![](Week3_files/figure-revealjs/cell-8-output-1.png){width=813 height=431}\n:::\n:::\n\n\n## Example 2\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nimport numpy as np, pandas as pd\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nimport matplotlib.pyplot as plt\nplt.rcParams.update({'figure.figsize':(9,7), 'figure.dpi':120})\n\n# Import data\ndf = pd.read_csv('wwwusage.csv', names=['value'], header=0)\n\n# Original Series\nfig, axes = plt.subplots(2, 2, sharex=True)\naxes[0, 0].plot(df.value); axes[0, 0].set_title('Original Series')\nplot_acf(df.value, ax=axes[0, 1], lags=np.arange(len(df)))\n\n# 1st Differencing\naxes[1, 0].plot(df.value.diff()); axes[1, 0].set_title('1st Order Differencing')\nplot_acf(df.value.diff().dropna(), ax=axes[1, 1], lags=np.arange(len(df) - 1))\nplt.show()\n```\n:::\n\n\n##\n\n::: {.cell execution_count=9}\n\n::: {.cell-output .cell-output-display}\n![](Week3_files/figure-revealjs/cell-10-output-1.png){}\n:::\n:::\n\n\n## 2nd order differencing\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nplot_acf(df.value.diff().diff().dropna())\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Week3_files/figure-revealjs/cell-11-output-1.png){}\n:::\n:::\n\n\n## Variance stabilization\n\nEg:\n\n- Square root: $W_t = \\sqrt{Y_t}$\n\n- Logarithm: $W_t = log({Y_t})$\n\n     - This very useful.\n     \n     - Interpretable: Changes in a log value are **relative (percent) changes on the original sclae**.\n     \n## Monthly Airline Passenger Numbers 1949-1960\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nairpassenger = pd.read_csv('AirPassengers.csv')\nfrom datetime import datetime\nimport plotnine\nfrom plotnine import *\nairpassenger['Month']= pd.to_datetime(airpassenger['Month'])\nggplot(airpassenger, aes(x='Month', y='#Passengers'))+geom_line()\n```\n\n::: {.cell-output .cell-output-display}\n![](Week3_files/figure-revealjs/cell-12-output-1.png){}\n:::\n\n::: {.cell-output .cell-output-display execution_count=38}\n```\n<ggplot: (335576854)>\n```\n:::\n:::\n\n\n## Monthly Airline Passenger Numbers 1949-1960 - log\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\nimport numpy as np\nairpassenger['naturallog'] = np.log(airpassenger['#Passengers']) \nggplot(airpassenger, aes(x='Month', y='naturallog'))+geom_line()\n```\n\n::: {.cell-output .cell-output-display}\n![](Week3_files/figure-revealjs/cell-13-output-1.png){}\n:::\n\n::: {.cell-output .cell-output-display execution_count=39}\n```\n<ggplot: (335535231)>\n```\n:::\n:::\n\n\n## Box-Cox transformation\n\n$$\n  w_t=\\begin{cases}\n    log(y_t), & \\text{if $\\lambda=0$} \\newline\n    (Y_t^\\lambda - 1)/ \\lambda, & \\text{otherwise}.\n  \\end{cases}\n$$\n\n\nDifferent values of $\\lambda$ gives you different transformations.\n\n- $\\lambda=1$: No **substantive** transformation\n\n- $\\lambda = \\frac{1}{2}$: Square root plus linear transformation\n\n- $\\lambda=0$: Natural logarithm\n\n- $\\lambda = -1$: Inverse plus 1\n\nBalance the seasonal fluctuations and random variation across the series.\n\n## Box-Cox transformation\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\n# import modules\nimport numpy as np\nfrom scipy import stats\n \ny2,fitted_lambda = stats.boxcox(airpassenger['#Passengers'])\n```\n:::\n\n\n##  Box-Cox transformation: Exploring the output\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nfitted_lambda\n```\n\n::: {.cell-output .cell-output-display execution_count=41}\n```\n0.14802265137037945\n```\n:::\n:::\n\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\ny2\n```\n\n::: {.cell-output .cell-output-display execution_count=42}\n```\narray([ 6.82749005,  6.93282224,  7.16189151,  7.11461078,  6.98378687,\n        7.20826542,  7.39959794,  7.39959794,  7.22352834,  6.94993188,\n        6.67930112,  6.93282224,  6.88074148,  7.0663838 ,  7.29843847,\n        7.20826542,  7.05009066,  7.41371485,  7.69297755,  7.69297755,\n        7.53726005,  7.17744836,  6.86312389,  7.28363955,  7.35675408,\n        7.42775127,  7.791663  ,  7.6033268 ,  7.71801394,  7.791663  ,\n        8.03379957,  8.03379957,  7.86322651,  7.59025293,  7.3711186 ,\n        7.64214252,  7.70552693,  7.81574285,  7.96693012,  7.82769741,\n        7.85143867,  8.23478523,  8.35415797,  8.46833738,  8.14152446,\n        7.94424651,  7.71801394,  7.97819691,  8.00058286,  8.00058286,\n        8.41186604,  8.40233549,  8.34441554,  8.47763304,  8.66568618,\n        8.73398286,  8.42136224,  8.16254066,  7.81574285,  8.05570781,\n        8.08822445,  7.90983871,  8.40233549,  8.32482145,  8.39277032,\n        8.66568618,  8.97573698,  8.90544371,  8.62209995,  8.34441554,\n        8.0774311 ,  8.34441554,  8.46833738,  8.38317027,  8.69150146,\n        8.70857469,  8.71707079,  9.07418456,  9.41661628,  9.30252389,\n        9.05177744,  8.75078932,  8.42136224,  8.78409104,  8.83328615,\n        8.77580407,  9.08902184,  9.0592668 ,  9.0964106 ,  9.48162515,\n        9.72179099,  9.67415098,  9.35679401,  9.00640692,  8.72554012,\n        9.00640692,  9.07418456,  8.96801544,  9.36350433,  9.30936559,\n        9.35679401,  9.77445522, 10.01359054, 10.02424732,  9.66813973,\n        9.30252389,  8.9987716 ,  9.22613489,  9.25415593,  9.0964106 ,\n        9.40343224,  9.30936559,  9.41003199,  9.84886109, 10.14918625,\n       10.21968352,  9.66813973,  9.38353935,  9.03673716,  9.23316669,\n        9.390186  ,  9.26806127,  9.68014959,  9.61958794,  9.76283534,\n       10.05072014, 10.426264  , 10.4768849 , 10.00289463,  9.68613564,\n        9.40343224,  9.67415098,  9.74531682,  9.58881702,  9.75700771,\n        9.99215929, 10.05072014, 10.36531089, 10.75145254, 10.68404894,\n       10.23457308,  9.99215929,  9.58262264,  9.83186035])\n```\n:::\n:::\n\n\n## ARMA(p, q) model\n\n\n$$Y_t=c+\\phi_1Y_{t-1}+...+\\phi_p Y_{t-p}+ \\theta_1\\epsilon_{t-1}+...+\\theta_q\\epsilon_{t-q}+\\epsilon_t$$\n\n- These are stationary models.\n\n- They are only suitable for **stationary series**.\n\n## ARIMA(p, d, q) model\n\nDifferencing --> ARMA\n\n**Step 1: Differencing**\n\n$$Y'_t = (1-B)^dY_t$$\n\n**Step 2: ARMA**\n\n$$Y'_t=c+\\phi_1Y'_{t-1}+...+\\phi_p Y'_{t-p}+ \\theta_1\\epsilon_{t-1}+...+\\theta_q\\epsilon_{t-q}+\\epsilon_t$$\n\n# Step 1: Plot data\n\n1. Detect unusual observations in the data\n\n1. Detect non-stationarity by visual inspections of plots\n\nStationary series:\n\n- has a constant mean value and fluctuates around the mean.\n\n- constant variance.\n\n- no pattern predictable in the long-term.\n\n##\n\n::: {.cell execution_count=16}\n\n::: {.cell-output .cell-output-display execution_count=43}\n```\n(<Figure size 1920x480 with 1 Axes>,\n <AxesSubplot: ylabel='Number of airline passengers'>)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](Week3_files/figure-revealjs/cell-17-output-2.png){}\n:::\n:::\n\n\n## Step 2: Split time series into training and test\n\nSpecify the forecast horizon\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\nfrom sktime.forecasting.base import ForecastingHorizon\nfh = ForecastingHorizon(\n    pd.PeriodIndex(pd.date_range(\"1960-01\", periods=12, freq=\"M\")), is_relative=False\n)\nfh\n```\n\n::: {.cell-output .cell-output-display execution_count=44}\n```\nForecastingHorizon(['1960-01', '1960-02', '1960-03', '1960-04', '1960-05', '1960-06',\n             '1960-07', '1960-08', '1960-09', '1960-10', '1960-11', '1960-12'],\n            dtype='period[M]', is_relative=False)\n```\n:::\n:::\n\n\n## Plot training and test series\n\n::: {.cell execution_count=18}\n\n::: {.cell-output .cell-output-display execution_count=45}\n```\n(<Figure size 1920x480 with 1 Axes>,\n <AxesSubplot: ylabel='Number of airline passengers'>)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](Week3_files/figure-revealjs/cell-19-output-2.png){}\n:::\n:::\n\n\n##\n\n1. Need transformations?\n\n2. Need differencing?\n\n## Step 3: Apply transformations\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\nimport numpy as np\ny_train.naturallog = np.log(y_train) \nplot_series(y_train.naturallog)\n```\n\n::: {.cell-output .cell-output-display execution_count=46}\n```\n(<Figure size 1920x480 with 1 Axes>,\n <AxesSubplot: ylabel='Number of airline passengers'>)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](Week3_files/figure-revealjs/cell-20-output-2.png){}\n:::\n:::\n\n\n## Step 4: Take difference series\n\n**Identifying non-stationarity by looking at plots**\n  \n- Time series plot\n\n- The ACF of stationary data drops to zero relatively quickly.\n\n- The ACF of non-stationary data decreases slowly.\n\n- For non-stationary data, the value of $r_1$ is often large and positive.\n\n## Non-seasonal differencing and seasonal differencing\n\n**Non seasonal first-order differencing:** $Y'_t=Y_t - Y_{t-1}$\n\n<!--Miss one observation-->\n\n**Non seasonal second-order differencing:** $Y''_t=Y'_t - Y'_{t-1}$\n\n<!--Miss two observations-->\n\n**Seasonal differencing:** $Y_t - Y_{t-m}$\n\n<!--To get rid from prominent seasonal components. -->\n\n- For monthly, $m=12$, for quarterly, $m=4$.\n\n<!--We will loosefirst 12 observations-->\n\n\n- Seasonally differenced series will have $T-m$ observations.\n<!--Usually we do not consider differencing more than twice. -->\n\n> There are times differencing once is not enough. However, in practice,it is almost never necessary to go beyond second-order differencing.\n<!--Even the second-order differencing is very rare.-->\n\n## ACF of log-transformation series\n\n::: {.cell execution_count=20}\n``` {.python .cell-code}\nplot_acf(y_train.naturallog, lags=50)\n```\n\n::: {.cell-output .cell-output-display execution_count=47}\n![](Week3_files/figure-revealjs/cell-21-output-1.png){}\n:::\n\n::: {.cell-output .cell-output-display}\n![](Week3_files/figure-revealjs/cell-21-output-2.png){}\n:::\n:::\n\n\n## Take seasonal difference series\n\n::: {.cell execution_count=21}\n``` {.python .cell-code}\ny_train.naturallog.diff12 = y_train.naturallog.diff(12)\ny_train.naturallog.diff12\n```\n\n::: {.cell-output .cell-output-display execution_count=48}\n```\n1949-01         NaN\n1949-02         NaN\n1949-03         NaN\n1949-04         NaN\n1949-05         NaN\n             ...   \n1959-08    0.101591\n1959-09    0.136312\n1959-10    0.125491\n1959-11    0.155072\n1959-12    0.183804\nFreq: M, Name: Number of airline passengers, Length: 132, dtype: float64\n```\n:::\n:::\n\n\n## Take seasonal difference series (cont.)\n\n::: {.cell execution_count=22}\n``` {.python .cell-code}\ny_train.naturallog.diff12.head(20)\n```\n\n::: {.cell-output .cell-output-display execution_count=49}\n```\n1949-01         NaN\n1949-02         NaN\n1949-03         NaN\n1949-04         NaN\n1949-05         NaN\n1949-06         NaN\n1949-07         NaN\n1949-08         NaN\n1949-09         NaN\n1949-10         NaN\n1949-11         NaN\n1949-12         NaN\n1950-01    0.026433\n1950-02    0.065597\n1950-03    0.065958\n1950-04    0.045462\n1950-05    0.032523\n1950-06    0.098672\n1950-07    0.138586\n1950-08    0.138586\nFreq: M, Name: Number of airline passengers, dtype: float64\n```\n:::\n:::\n\n\n## ACF - diff(log(data), 12)\n\n::: {.cell execution_count=23}\n``` {.python .cell-code}\nplot_acf(y_train.naturallog.diff12.dropna(), lags=50)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Week3_files/figure-revealjs/cell-24-output-1.png){}\n:::\n:::\n\n\n## ACF - First differencing on diff(log(data), 12)\n\n::: {.cell execution_count=24}\n``` {.python .cell-code}\ny_train.naturallog.diff12.diff = y_train.naturallog.diff12.diff()\nplot_acf(y_train.naturallog.diff12.diff.dropna(), lags=50)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Week3_files/figure-revealjs/cell-25-output-1.png){}\n:::\n:::\n\n\n## PACF - First differencing on diff(log(data), 12)\n\n::: {.cell execution_count=25}\n``` {.python .cell-code}\nplot_pacf(y_train.naturallog.diff12.diff.dropna(), lags=50)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Week3_files/figure-revealjs/cell-26-output-1.png){}\n:::\n:::\n\n\n# Step 5: Examine the ACF/PACF to identify a suitable model\n\n## AR(p)\n\n- ACF dies out in an exponential or damped\nsine-wave manner.\n\n- there is a significant spike at lag $p$ in PACF, but\nnone beyond $p$.\n\n## MA(q)\n\n- ACF has all zero spikes beyond the $q^{th}$ spike.\n\n- PACF dies out in an exponential or damped\nsine-wave manner.\n\n## Seasonal components\n\n- The seasonal part of an AR or MA model will be seen\nin the seasonal lags of the PACF and ACF.\n\n\n## ARIMA(0,0,0)(0,0,1)12 will show\n \n  - a spike at lag 12 in the ACF but no other significant spikes.\n\n  - The PACF will show exponential decay in the seasonal lags  12, 24, 36, . . . .\n  \n## ARIMA(0,0,0)(1,0,0)12 will show\n\n  - exponential decay in the seasonal lags of the ACF.\n    \n  - a single significant spike at lag 12 in the PACF.\n\n\n## Step 5: Examine the ACF/PACF to identify a suitable model (cont.)\n\n- $d=1$ and $D=1$ (from step 4)\n\n- Significant spike at lag 1 in ACF suggests\nnon-seasonal MA(1) component.\n\n- Significant spike at lag 12 in ACF suggests seasonal\nMA(1) component.\n\n- Initial candidate model: $ARIMA(0,1,1)(0,1,1)_{12}$.\n\n- By analogous logic applied to the PACF, we could also have started with $ARIMA(1,1,0)(1,1,0)_{12}$.\n  \n## Models   \n  \n**Initial model:**\n\n$ARIMA(0,1,1)(0,1,1)_{12}$\n\n$ARIMA(1,1,0)(1,1,0)_{12}$\n\n**Try some variations of the initial model:**\n\n$ARIMA(0,1,1)(1,1,1)_{12}$\n\n$ARIMA(1,1,1)(1,1,0)_{12}$\n\n$ARIMA(1,1,1)(1,1,1)_{12}$\n\n##\n\n**Try some variations**\n\nBoth the ACF and PACF show significant spikes at lag 3, and almost significant spikes at lag 3, indicating that some additional non-seasonal terms need to be included in the model.\n\n$ARIMA(3,1,1)(1,1,1)_{12}$\n\n$ARIMA(1,1,3)(1,1,1)_{12}$\n\n$ARIMA(3,1,3)(1,1,1)_{12}$\n\n## Fitting ARIMA models\n\n::: {.cell execution_count=26}\n``` {.python .cell-code}\nfrom sktime.forecasting.arima import ARIMA\nforecaster1 = ARIMA(  \n    order=(0, 1, 1),\n    seasonal_order=(0, 1, 1, 12),\n    suppress_warnings=True)\nforecaster1.fit(y_train.naturallog)    \ny_pred_1 = forecaster1.predict(fh)\ny_pred_1\n```\n\n::: {.cell-output .cell-output-display execution_count=53}\n```\n1960-01    6.038700\n1960-02    5.988821\n1960-03    6.145521\n1960-04    6.119111\n1960-05    6.159829\n1960-06    6.304860\n1960-07    6.433534\n1960-08    6.446252\n1960-09    6.266993\n1960-10    6.136496\n1960-11    6.008236\n1960-12    6.114690\nFreq: M, dtype: float64\n```\n:::\n:::\n\n\n## Back transformation\n\n::: {.cell execution_count=27}\n``` {.python .cell-code}\ny_pred_1.exp = np.exp(y_pred_1)\ny_pred_1.exp\n```\n\n::: {.cell-output .cell-output-display execution_count=54}\n```\n1960-01    419.347465\n1960-02    398.944005\n1960-03    466.622813\n1960-04    454.460279\n1960-05    473.347301\n1960-06    547.225107\n1960-07    622.369714\n1960-08    630.335278\n1960-09    526.890634\n1960-10    462.430506\n1960-11    406.765291\n1960-12    452.455879\nFreq: M, dtype: float64\n```\n:::\n:::\n\n\n## Plot training, test, and forecasts\n\n::: {.cell execution_count=28}\n``` {.python .cell-code}\nplot_series(y_train, y_test, y_pred_1.exp, labels=[\"y_train\", \"y_test\", \"y_forecast\"])\n```\n\n::: {.cell-output .cell-output-display execution_count=55}\n```\n(<Figure size 1920x480 with 1 Axes>,\n <AxesSubplot: ylabel='Number of airline passengers'>)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](Week3_files/figure-revealjs/cell-29-output-2.png){}\n:::\n:::\n\n\n## Evaluation\n\n::: {.cell execution_count=29}\n``` {.python .cell-code}\nfrom sktime.performance_metrics.forecasting import \\\n    mean_absolute_percentage_error\nmean_absolute_percentage_error(y_test, y_pred_1.exp, symmetric=False)\n```\n\n::: {.cell-output .cell-output-display execution_count=56}\n```\n0.029239695342961447\n```\n:::\n:::\n\n\n## Your Turn\n\nFit other variants of ARIMA models and identify the best ARIMA model for the series.\n\n",
    "supporting": [
      "Week3_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}