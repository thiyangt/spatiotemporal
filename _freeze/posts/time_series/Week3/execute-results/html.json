{
  "hash": "12f897f6605743924949a352b7b9d728",
  "result": {
    "markdown": "---\ntitle: 'Week 3: AR/ MA/ ARMA/ ARIMA'\nformat:\n  revealjs:\n    slide-number: true\n    show-slide-number: all\n---\n\n# Recap: Stationarity\n\n::: {.cell .fig-column-margin execution_count=1}\n``` {.python .cell-code}\nimport numpy\nimport matplotlib.pyplot as plt\n\nmean = 0\nstd = 1 \nnum_samples = 100\nsamples = numpy.random.normal(mean, std, size=num_samples)\nplt.plot(samples)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Week3_files/figure-revealjs/cell-2-output-1.png){width=792 height=411}\n:::\n:::\n\n\n## ACF \n\n::: {.cell execution_count=2}\n\n::: {.cell-output .cell-output-display}\n![](Week3_files/figure-revealjs/cell-3-output-1.png){width=813 height=431}\n:::\n:::\n\n\nWhite noise implies stationarity. Stationarity does not imply white noise.\n\n## Non-Stationary Time Series\n\n**1. Deterministic trend**\n\n$$Y_t  = f(t) + \\epsilon_t$$\n\n\nwhere $\\epsilon_t \\sim iid(0, \\sigma^2)$, $t = 1, 2, ...T$\n\nMean of the process is time dependent, but the variance of the process is constant.\n\nA trend is deterministic if it is a nonrandom function of time.\n\n## Non-Stationary Time Series (cont.)\n\n**2. Random walk** \n\n$$Y_t = Y_{t-1} + \\epsilon_t$$\n\n- Random walk has a stochastic trend.\n\n- Model behind naive method.\n\nA trend is said to be stochastic if it is a random function of time.\n\n\n\n## Non-Stationary Time Series (cont.)\n\n**3. Random walk with drift**\n\n$$Y_t = \\alpha+  Y_{t-1} + \\epsilon_t$$\n\n- Random walk with drift has a stochastic trend and a deterministic trend.\n\n- Model behind drift method.\n\n\n## Random walk\n\n\n$$\n\\begin{aligned}\n  Y_t &= Y_{t-1} + \\epsilon_t \\\\\n     Y_1    &= Y_0 + \\epsilon_1 \\\\\n         Y_2 &=  Y_1 + \\epsilon_2=Y_0 + \\epsilon_1 + \\epsilon_2\\\\\n          Y_3 &=  Y_2 + \\epsilon_3=Y_0 + \\epsilon_1 + \\epsilon_2 +\\epsilon_3\\\\\n          .   \\\\\n          Y_t &=Y_{t-1} + \\epsilon_t=Y_0 + \\epsilon_1 + \\epsilon_2 + \\epsilon_3 +...+ \\epsilon_t = Y_0 + \\sum_{i=1}^{t} \\epsilon_t\n\\end{aligned}\n$$\n\nMean: $E(Y_t) = Y_0$.\n\nVariance: $Var(Y_t)=t \\sigma^2$.\n\n## Random walk with drift\n\n\n$$\n\\begin{aligned}\n  Y_t &= Y_{t-1} + \\epsilon_t \\\\\n     Y_1    &= \\alpha+Y_0 + \\epsilon_1 \\\\\n         Y_2 &= \\alpha+ Y_1 + \\epsilon_2=2 \\alpha+Y_0 + \\epsilon_1 + \\epsilon_2\\\\\n          Y_3 &= \\alpha+ Y_2 + \\epsilon_3= 3 \\alpha+ Y_0 + \\epsilon_1 + \\epsilon_2 +\\epsilon_3\\\\\n          .   \\\\\n          Y_t &= \\alpha+Y_{t-1} + \\epsilon_t= t \\alpha+ Y_0 + \\epsilon_1 + \\epsilon_2 + \\epsilon_3 +...+ \\epsilon_t \\\\\n          Y_t &= t \\alpha + Y_0 + \\sum_{i=1}^{t} \\epsilon_t\n\\end{aligned}\n$$\n\n## Random walk with drift (cont.)\n\n\nIt has a *deterministic trend* $(Y_0 + t \\alpha)$ and a *stochastic trend* $\\sum_{i=1}^{t} \\epsilon_t$.\n\nMean: $E(Y_t) = Y_0 + t\\alpha$\n\nVariance: $Var(Y_t) = t\\sigma^2$.\n\nThere is a trend in both mean and variance. \n\n\n## Common trend removal (de-trending) procedures\n\n1. Deterministic trend: Time-trend regression\n\n      The trend can be removed by fitting a deterministic polynomial time trend. The residual series after removing the trend will give us the de-trended series.\n\n1. Stochastic trend: Differencing\n \n      The process is also known as a **Difference-stationary process**.\n      \n# Notation: I(d)\n\nIntegrated to order $d$: Series can be made stationary by differencing $d$ times.\n \n - Known as $I(d)$ process.\n \n\n**Question: ** Show that random walk process is an $I(1)$ process.\n\nThe random walk process is called a unit root process.\n(If one of the roots turns out to be one, then the process is called unit root process.)\n\n## Random walk\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nimport numpy as np\nrw = np.cumsum(samples)\nplt.plot(rw)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Week3_files/figure-revealjs/cell-4-output-1.png){width=801 height=411}\n:::\n:::\n\n\n## Random walk - ACF\n\n::: {.cell execution_count=4}\n\n::: {.cell-output .cell-output-display}\n![](Week3_files/figure-revealjs/cell-5-output-1.png){width=813 height=431}\n:::\n:::\n\n\n## Difference series\n\n::: {.cell execution_count=5}\n\n::: {.cell-output .cell-output-display execution_count=324}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Values</th>\n      <th>Lag 1</th>\n      <th>Lag 2</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.138088</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.752350</td>\n      <td>-0.890438</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.449339</td>\n      <td>1.201689</td>\n      <td>2.092127</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.625002</td>\n      <td>1.175663</td>\n      <td>-0.026026</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.149277</td>\n      <td>-0.475725</td>\n      <td>-1.651387</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>-7.914790</td>\n      <td>0.768559</td>\n      <td>2.291945</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>-7.822851</td>\n      <td>0.091939</td>\n      <td>-0.676620</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>-7.561104</td>\n      <td>0.261747</td>\n      <td>0.169808</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>-7.702713</td>\n      <td>-0.141608</td>\n      <td>-0.403355</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>-7.326441</td>\n      <td>0.376272</td>\n      <td>0.517880</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows Ã— 3 columns</p>\n</div>\n```\n:::\n:::\n\n\n## Plot Lag 1 series\n\n::: {.cell execution_count=6}\n\n::: {.cell-output .cell-output-display}\n![](Week3_files/figure-revealjs/cell-7-output-1.png){width=792 height=411}\n:::\n:::\n\n\n## ACF Lag 1 series\n\n::: {.cell execution_count=7}\n\n::: {.cell-output .cell-output-display}\n![](Week3_files/figure-revealjs/cell-8-output-1.png){width=813 height=431}\n:::\n:::\n\n\n## Example 2\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nimport numpy as np, pandas as pd\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nimport matplotlib.pyplot as plt\nplt.rcParams.update({'figure.figsize':(9,7), 'figure.dpi':120})\n\n# Import data\ndf = pd.read_csv('wwwusage.csv', names=['value'], header=0)\n\n# Original Series\nfig, axes = plt.subplots(2, 2, sharex=True)\naxes[0, 0].plot(df.value); axes[0, 0].set_title('Original Series')\nplot_acf(df.value, ax=axes[0, 1], lags=np.arange(len(df)))\n\n# 1st Differencing\naxes[1, 0].plot(df.value.diff()); axes[1, 0].set_title('1st Order Differencing')\nplot_acf(df.value.diff().dropna(), ax=axes[1, 1], lags=np.arange(len(df) - 1))\nplt.show()\n```\n:::\n\n\n##\n\n::: {.cell execution_count=9}\n\n::: {.cell-output .cell-output-display}\n![](Week3_files/figure-revealjs/cell-10-output-1.png){}\n:::\n:::\n\n\n## 2nd order differencing\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nplot_acf(df.value.diff().diff().dropna())\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Week3_files/figure-revealjs/cell-11-output-1.png){}\n:::\n:::\n\n\n## Variance stabilization\n\nEg:\n\n- Square root: $W_t = \\sqrt{Y_t}$\n\n- Logarithm: $W_t = log({Y_t})$\n\n     - This very useful.\n     \n     - Interpretable: Changes in a log value are **relative (percent) changes on the original sclae**.\n     \n## Monthly Airline Passenger Numbers 1949-1960\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nairpassenger = pd.read_csv('AirPassengers.csv')\nfrom datetime import datetime\nimport plotnine\nfrom plotnine import *\nairpassenger['Month']= pd.to_datetime(airpassenger['Month'])\nggplot(airpassenger, aes(x='Month', y='#Passengers'))+geom_line()\n```\n\n::: {.cell-output .cell-output-display}\n![](Week3_files/figure-revealjs/cell-12-output-1.png){}\n:::\n\n::: {.cell-output .cell-output-display execution_count=329}\n```\n<ggplot: (329317690)>\n```\n:::\n:::\n\n\n## Monthly Airline Passenger Numbers 1949-1960 - log\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\nimport numpy as np\nairpassenger['naturallog'] = np.log(airpassenger['#Passengers']) \nggplot(airpassenger, aes(x='Month', y='naturallog'))+geom_line()\n```\n\n::: {.cell-output .cell-output-display}\n![](Week3_files/figure-revealjs/cell-13-output-1.png){}\n:::\n\n::: {.cell-output .cell-output-display execution_count=330}\n```\n<ggplot: (326073878)>\n```\n:::\n:::\n\n\n## Box-Cox transformation\n\n$$\n  w_t=\\begin{cases}\n    log(y_t), & \\text{if $\\lambda=0$} \\newline\n    (Y_t^\\lambda - 1)/ \\lambda, & \\text{otherwise}.\n  \\end{cases}\n$$\n\n\nDifferent values of $\\lambda$ gives you different transformations.\n\n- $\\lambda=1$: No **substantive** transformation\n\n- $\\lambda = \\frac{1}{2}$: Square root plus linear transformation\n\n- $\\lambda=0$: Natural logarithm\n\n- $\\lambda = -1$: Inverse plus 1\n\nBalance the seasonal fluctuations and random variation across the series.\n\n## Box-Cox transformation\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\n# import modules\nimport numpy as np\nfrom scipy import stats\n \ny2,fitted_lambda = stats.boxcox(airpassenger['#Passengers'])\n```\n:::\n\n\n##  Box-Cox transformation: Exploring the output\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nfitted_lambda\n```\n\n::: {.cell-output .cell-output-display execution_count=332}\n```\n0.14802265137037945\n```\n:::\n:::\n\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\ny2\n```\n\n::: {.cell-output .cell-output-display execution_count=333}\n```\narray([ 6.82749005,  6.93282224,  7.16189151,  7.11461078,  6.98378687,\n        7.20826542,  7.39959794,  7.39959794,  7.22352834,  6.94993188,\n        6.67930112,  6.93282224,  6.88074148,  7.0663838 ,  7.29843847,\n        7.20826542,  7.05009066,  7.41371485,  7.69297755,  7.69297755,\n        7.53726005,  7.17744836,  6.86312389,  7.28363955,  7.35675408,\n        7.42775127,  7.791663  ,  7.6033268 ,  7.71801394,  7.791663  ,\n        8.03379957,  8.03379957,  7.86322651,  7.59025293,  7.3711186 ,\n        7.64214252,  7.70552693,  7.81574285,  7.96693012,  7.82769741,\n        7.85143867,  8.23478523,  8.35415797,  8.46833738,  8.14152446,\n        7.94424651,  7.71801394,  7.97819691,  8.00058286,  8.00058286,\n        8.41186604,  8.40233549,  8.34441554,  8.47763304,  8.66568618,\n        8.73398286,  8.42136224,  8.16254066,  7.81574285,  8.05570781,\n        8.08822445,  7.90983871,  8.40233549,  8.32482145,  8.39277032,\n        8.66568618,  8.97573698,  8.90544371,  8.62209995,  8.34441554,\n        8.0774311 ,  8.34441554,  8.46833738,  8.38317027,  8.69150146,\n        8.70857469,  8.71707079,  9.07418456,  9.41661628,  9.30252389,\n        9.05177744,  8.75078932,  8.42136224,  8.78409104,  8.83328615,\n        8.77580407,  9.08902184,  9.0592668 ,  9.0964106 ,  9.48162515,\n        9.72179099,  9.67415098,  9.35679401,  9.00640692,  8.72554012,\n        9.00640692,  9.07418456,  8.96801544,  9.36350433,  9.30936559,\n        9.35679401,  9.77445522, 10.01359054, 10.02424732,  9.66813973,\n        9.30252389,  8.9987716 ,  9.22613489,  9.25415593,  9.0964106 ,\n        9.40343224,  9.30936559,  9.41003199,  9.84886109, 10.14918625,\n       10.21968352,  9.66813973,  9.38353935,  9.03673716,  9.23316669,\n        9.390186  ,  9.26806127,  9.68014959,  9.61958794,  9.76283534,\n       10.05072014, 10.426264  , 10.4768849 , 10.00289463,  9.68613564,\n        9.40343224,  9.67415098,  9.74531682,  9.58881702,  9.75700771,\n        9.99215929, 10.05072014, 10.36531089, 10.75145254, 10.68404894,\n       10.23457308,  9.99215929,  9.58262264,  9.83186035])\n```\n:::\n:::\n\n\n## ARMA(p, q) model\n\n\n$$Y_t=c+\\phi_1Y_{t-1}+...+\\phi_p Y_{t-p}+ \\theta_1\\epsilon_{t-1}+...+\\theta_q\\epsilon_{t-q}+\\epsilon_t$$\n\n- These are stationary models.\n\n- They are only suitable for **stationary series**.\n\n## ARIMA(p, d, q) model\n\nDifferencing --> ARMA\n\n**Step 1: Differencing**\n\n$$Y'_t = (1-B)^dY_t$$\n\n**Step 2: ARMA**\n\n$$Y'_t=c+\\phi_1Y'_{t-1}+...+\\phi_p Y'_{t-p}+ \\theta_1\\epsilon_{t-1}+...+\\theta_q\\epsilon_{t-q}+\\epsilon_t$$\n\n# Step 1: Plot data\n\n1. Detect unusual observations in the data\n\n1. Detect non-stationarity by visual inspections of plots\n\nStationary series:\n\n- has a constant mean value and fluctuates around the mean.\n\n- constant variance.\n\n- no pattern predictable in the long-term.\n\n##\n\n::: {.cell execution_count=16}\n\n::: {.cell-output .cell-output-display execution_count=334}\n```\n(<Figure size 1920x480 with 1 Axes>,\n <AxesSubplot: ylabel='Number of airline passengers'>)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](Week3_files/figure-revealjs/cell-17-output-2.png){}\n:::\n:::\n\n\n## Step 2: Split time series into training and test\n\nSpecify the forecast horizon\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\nfrom sktime.forecasting.base import ForecastingHorizon\nfh = ForecastingHorizon(\n    pd.PeriodIndex(pd.date_range(\"1960-01\", periods=12, freq=\"M\")), is_relative=False\n)\nfh\n```\n\n::: {.cell-output .cell-output-display execution_count=335}\n```\nForecastingHorizon(['1960-01', '1960-02', '1960-03', '1960-04', '1960-05', '1960-06',\n             '1960-07', '1960-08', '1960-09', '1960-10', '1960-11', '1960-12'],\n            dtype='period[M]', is_relative=False)\n```\n:::\n:::\n\n\n## Plot training and test series\n\n::: {.cell execution_count=18}\n\n::: {.cell-output .cell-output-display execution_count=336}\n```\n(<Figure size 1920x480 with 1 Axes>,\n <AxesSubplot: ylabel='Number of airline passengers'>)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](Week3_files/figure-revealjs/cell-19-output-2.png){}\n:::\n:::\n\n\n##\n\n1. Need transformations?\n\n2. Need differencing?\n\n## Step 3: Apply transformations\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\nimport numpy as np\ny_train.naturallog = np.log(y_train) \nplot_series(y_train.naturallog)\n```\n\n::: {.cell-output .cell-output-display execution_count=337}\n```\n(<Figure size 1920x480 with 1 Axes>,\n <AxesSubplot: ylabel='Number of airline passengers'>)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](Week3_files/figure-revealjs/cell-20-output-2.png){}\n:::\n:::\n\n\n## Step 4: Take difference series\n\n**Identifying non-stationarity by looking at plots**\n  \n- Time series plot\n\n- The ACF of stationary data drops to zero relatively quickly.\n\n- The ACF of non-stationary data decreases slowly.\n\n- For non-stationary data, the value of $r_1$ is often large and positive.\n\n## Non-seasonal differencing and seasonal differencing\n\n**Non seasonal first-order differencing:** $Y'_t=Y_t - Y_{t-1}$\n\n<!--Miss one observation-->\n\n**Non seasonal second-order differencing:** $Y''_t=Y'_t - Y'_{t-1}$\n\n<!--Miss two observations-->\n\n**Seasonal differencing:** $Y_t - Y_{t-m}$\n\n<!--To get rid from prominent seasonal components. -->\n\n- For monthly, $m=12$, for quarterly, $m=4$.\n\n<!--We will loosefirst 12 observations-->\n\n\n- Seasonally differenced series will have $T-m$ observations.\n<!--Usually we do not consider differencing more than twice. -->\n\n> There are times differencing once is not enough. However, in practice,it is almost never necessary to go beyond second-order differencing.\n<!--Even the second-order differencing is very rare.-->\n\n## ACF of log-transformation series\n\n::: {.cell execution_count=20}\n``` {.python .cell-code}\nplot_acf(y_train.naturallog, lags=50)\n```\n\n::: {.cell-output .cell-output-display execution_count=338}\n![](Week3_files/figure-revealjs/cell-21-output-1.png){}\n:::\n\n::: {.cell-output .cell-output-display}\n![](Week3_files/figure-revealjs/cell-21-output-2.png){}\n:::\n:::\n\n\n## Take seasonal difference series\n\n::: {.cell execution_count=21}\n``` {.python .cell-code}\ny_train.naturallog.diff12 = y_train.naturallog.diff(12)\ny_train.naturallog.diff12\n```\n\n::: {.cell-output .cell-output-display execution_count=339}\n```\n1949-01         NaN\n1949-02         NaN\n1949-03         NaN\n1949-04         NaN\n1949-05         NaN\n             ...   \n1959-08    0.101591\n1959-09    0.136312\n1959-10    0.125491\n1959-11    0.155072\n1959-12    0.183804\nFreq: M, Name: Number of airline passengers, Length: 132, dtype: float64\n```\n:::\n:::\n\n\n## Take seasonal difference series (cont.)\n\n::: {.cell execution_count=22}\n``` {.python .cell-code}\ny_train.naturallog.diff12.head(20)\n```\n\n::: {.cell-output .cell-output-display execution_count=340}\n```\n1949-01         NaN\n1949-02         NaN\n1949-03         NaN\n1949-04         NaN\n1949-05         NaN\n1949-06         NaN\n1949-07         NaN\n1949-08         NaN\n1949-09         NaN\n1949-10         NaN\n1949-11         NaN\n1949-12         NaN\n1950-01    0.026433\n1950-02    0.065597\n1950-03    0.065958\n1950-04    0.045462\n1950-05    0.032523\n1950-06    0.098672\n1950-07    0.138586\n1950-08    0.138586\nFreq: M, Name: Number of airline passengers, dtype: float64\n```\n:::\n:::\n\n\n## ACF - diff(log(data), 12)\n\n::: {.cell execution_count=23}\n``` {.python .cell-code}\nplot_acf(y_train.naturallog.diff12.dropna(), lags=50)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Week3_files/figure-revealjs/cell-24-output-1.png){}\n:::\n:::\n\n\n## ACF - First differencing on diff(log(data), 12)\n\n::: {.cell execution_count=24}\n``` {.python .cell-code}\ny_train.naturallog.diff12.diff = y_train.naturallog.diff12.diff()\nplot_acf(y_train.naturallog.diff12.diff.dropna(), lags=50)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Week3_files/figure-revealjs/cell-25-output-1.png){}\n:::\n:::\n\n\n## PACF - First differencing on diff(log(data), 12)\n\n::: {.cell execution_count=25}\n``` {.python .cell-code}\nplot_pacf(y_train.naturallog.diff12.diff.dropna(), lags=50)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Week3_files/figure-revealjs/cell-26-output-1.png){}\n:::\n:::\n\n\n## Testing for nonstationarity for the presence of unit roots\n\n- Dickey and Fuller (DF) test\n\n- Augmented DF test\n\n- Phillips and Perron (PP) nonparametric test\n\n- Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test\n\n## KPSS test\nH0: Series is level or trend stationary.\n\nH1: Series is not stationary.\n\n## KPSS test\n\n::: {.cell execution_count=26}\n``` {.python .cell-code}\nfrom statsmodels.tsa.stattools import kpss\ndef kpss_test(series, **kw):    \n    statistic, p_value, n_lags, critical_values = kpss(series, **kw)\n    # Format Output\n    print(f'KPSS Statistic: {statistic}')\n    print(f'p-value: {p_value}')\n    print(f'num lags: {n_lags}')\n    print('Critial Values:')\n    for key, value in critical_values.items():\n        print(f'   {key} : {value}')\n    print(f'Result: The series is {\"not \" if p_value < 0.05 else \"\"}stationary')\n\nkpss_test(y_train.naturallog)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nKPSS Statistic: 1.9204939010623039\np-value: 0.01\nnum lags: 6\nCritial Values:\n   10% : 0.347\n   5% : 0.463\n   2.5% : 0.574\n   1% : 0.739\nResult: The series is not stationary\n```\n:::\n:::\n\n\n## KPSS test\n\n::: {.cell execution_count=27}\n``` {.python .cell-code}\nkpss_test(y_train.naturallog.diff12.dropna())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nKPSS Statistic: 0.29885781439314946\np-value: 0.1\nnum lags: 5\nCritial Values:\n   10% : 0.347\n   5% : 0.463\n   2.5% : 0.574\n   1% : 0.739\nResult: The series is stationary\n```\n:::\n:::\n\n\n::: {.cell execution_count=28}\n``` {.python .cell-code}\nkpss_test(y_train.naturallog.diff12.diff.dropna())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nKPSS Statistic: 0.0509726778456186\np-value: 0.1\nnum lags: 2\nCritial Values:\n   10% : 0.347\n   5% : 0.463\n   2.5% : 0.574\n   1% : 0.739\nResult: The series is stationary\n```\n:::\n:::\n\n\n## KPSS test\n\n- KPSS test may not necessarily reject the null hypothesis (that the series is level or trend stationary) even if a series is steadily increasing or decreasing.\n\n- The word â€˜deterministicâ€™ implies the slope of the trend in the series does not change permanently. That is, even if the series goes through a shock, it tends to regain its original path.\n\nsource: https://www.machinelearningplus.com/time-series/kpss-test-for-stationarity/\n\n## KPSS test\n\n\n- By default, it tests for stationarity around a â€˜meanâ€™ only.\n\n- To turn ON the stationarity testing around a trend, you need to explicitly pass the regression='ct' parameter to the kpss\n\n::: {.cell execution_count=29}\n``` {.python .cell-code}\nkpss_test(y_train.naturallog.diff12.dropna(), regression='ct')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nKPSS Statistic: 0.0760056301424143\np-value: 0.1\nnum lags: 5\nCritial Values:\n   10% : 0.119\n   5% : 0.146\n   2.5% : 0.176\n   1% : 0.216\nResult: The series is stationary\n```\n:::\n:::\n\n\n::: {.cell execution_count=30}\n``` {.python .cell-code}\nkpss_test(y_train.naturallog.diff12.diff.dropna())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nKPSS Statistic: 0.0509726778456186\np-value: 0.1\nnum lags: 2\nCritial Values:\n   10% : 0.347\n   5% : 0.463\n   2.5% : 0.574\n   1% : 0.739\nResult: The series is stationary\n```\n:::\n:::\n\n\n## ADF test\n\n::: {.cell execution_count=31}\n``` {.python .cell-code}\nfrom statsmodels.tsa.stattools import adfuller\n\ndef adf_test(series):\n    result = adfuller(series, autolag='AIC')\n    print(f'ADF Statistic: {result[0]}')\n    print(f'p-value: {result[1]}')\n    for key, value in result[4].items():\n        print('Critial Values:')\n        print(f'   {key}, {value}')\n\nseries = df.loc[:, 'value'].values\n\n```\n:::\n\n\nH0: Series is not stationary\n\nH1: Series is stationary\n\n## ADF test\n\n::: {.cell execution_count=32}\n``` {.python .cell-code}\nadf_test(y_train.naturallog)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nADF Statistic: -1.3176112021439967\np-value: 0.6210771494355872\nCritial Values:\n   1%, -3.4870216863700767\nCritial Values:\n   5%, -2.8863625166643136\nCritial Values:\n   10%, -2.580009026141913\n```\n:::\n:::\n\n\n::: {.cell execution_count=33}\n``` {.python .cell-code}\nadf_test(y_train.naturallog.diff12.dropna())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nADF Statistic: -2.5844902417566793\np-value: 0.09624537566648711\nCritial Values:\n   1%, -3.492995948509562\nCritial Values:\n   5%, -2.888954648057252\nCritial Values:\n   10%, -2.58139291903223\n```\n:::\n:::\n\n\n::: {.cell execution_count=34}\n``` {.python .cell-code}\nadf_test(y_train.naturallog.diff12.diff.dropna())\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nADF Statistic: -4.08727195454389\np-value: 0.0010165214009067135\nCritial Values:\n   1%, -3.4936021509366793\nCritial Values:\n   5%, -2.8892174239808703\nCritial Values:\n   10%, -2.58153320754717\n```\n:::\n:::\n\n\n## KPSS vs ADF test\n\nIf a series is stationary according to the KPSS test by setting regression='ct' and is not stationary according to the ADF test, it means the series is stationary around a deterministic trend.\n\nFurther reading: \n\nKwiatkowski, D.; Phillips, P. C. B.; Schmidt, P.; Shin, Y. (1992). Testing the null hypothesis of stationarity against the alternative of a unit root. Journal of Econometrics, 54 (1-3): 159-178.\n\n# Step 5: Examine the ACF/PACF to identify a suitable model\n\n## AR(p)\n\n- ACF dies out in an exponential or damped\nsine-wave manner.\n\n- there is a significant spike at lag $p$ in PACF, but\nnone beyond $p$.\n\n## MA(q)\n\n- ACF has all zero spikes beyond the $q^{th}$ spike.\n\n- PACF dies out in an exponential or damped\nsine-wave manner.\n\n## Seasonal components\n\n- The seasonal part of an AR or MA model will be seen\nin the seasonal lags of the PACF and ACF.\n\n\n## ARIMA(0,0,0)(0,0,1)12 will show\n \n  - a spike at lag 12 in the ACF but no other significant spikes.\n\n  - The PACF will show exponential decay in the seasonal lags  12, 24, 36, . . . .\n  \n## ARIMA(0,0,0)(1,0,0)12 will show\n\n  - exponential decay in the seasonal lags of the ACF.\n    \n  - a single significant spike at lag 12 in the PACF.\n\n\n## Step 5: Examine the ACF/PACF to identify a suitable model (cont.)\n\n- $d=1$ and $D=1$ (from step 4)\n\n- Significant spike at lag 1 in ACF suggests\nnon-seasonal MA(1) component.\n\n- Significant spike at lag 12 in ACF suggests seasonal\nMA(1) component.\n\n- Initial candidate model: $ARIMA(0,1,1)(0,1,1)_{12}$.\n\n- By analogous logic applied to the PACF, we could also have started with $ARIMA(1,1,0)(1,1,0)_{12}$.\n  \n## Models   \n  \n**Initial model:**\n\n$ARIMA(0,1,1)(0,1,1)_{12}$\n\n$ARIMA(1,1,0)(1,1,0)_{12}$\n\n**Try some variations of the initial model:**\n\n$ARIMA(0,1,1)(1,1,1)_{12}$\n\n$ARIMA(1,1,1)(1,1,0)_{12}$\n\n$ARIMA(1,1,1)(1,1,1)_{12}$\n\n##\n\n**Try some variations**\n\nBoth the ACF and PACF show significant spikes at lag 3, and almost significant spikes at lag 3, indicating that some additional non-seasonal terms need to be included in the model.\n\n$ARIMA(3,1,1)(1,1,1)_{12}$\n\n$ARIMA(1,1,3)(1,1,1)_{12}$\n\n$ARIMA(3,1,3)(1,1,1)_{12}$\n\n## Fitting ARIMA models\n\n::: {.cell execution_count=35}\n``` {.python .cell-code}\nfrom sktime.forecasting.arima import ARIMA\nforecaster1 = ARIMA(  \n    order=(1, 1, 0),\n    seasonal_order=(1, 1, 0, 12),\n    suppress_warnings=True)\nforecaster1.fit(y_train.naturallog)    \n```\n\n::: {.cell-output .cell-output-display execution_count=353}\n```{=html}\n<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>ARIMA(order=(1, 1, 0), seasonal_order=(1, 1, 0, 12), suppress_warnings=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ARIMA</label><div class=\"sk-toggleable__content\"><pre>ARIMA(order=(1, 1, 0), seasonal_order=(1, 1, 0, 12), suppress_warnings=True)</pre></div></div></div></div></div>\n```\n:::\n:::\n\n\n## Step 6: Check residual series\n\n::: {.cell execution_count=36}\n``` {.python .cell-code}\nfhtrain = ForecastingHorizon(\n    pd.PeriodIndex(pd.period_range(start='1949-01', end='1959-12', freq='M')), is_relative=False\n)\nfhtrain\n```\n\n::: {.cell-output .cell-output-display execution_count=354}\n```\nForecastingHorizon(['1949-01', '1949-02', '1949-03', '1949-04', '1949-05', '1949-06',\n             '1949-07', '1949-08', '1949-09', '1949-10',\n             ...\n             '1959-03', '1959-04', '1959-05', '1959-06', '1959-07', '1959-08',\n             '1959-09', '1959-10', '1959-11', '1959-12'],\n            dtype='period[M]', length=132, is_relative=False)\n```\n:::\n:::\n\n\n## Obtain predictions for the training period\n\n::: {.cell execution_count=37}\n``` {.python .cell-code}\ny_pred_train = forecaster1.predict(fhtrain)\ny_pred_train\n```\n\n::: {.cell-output .cell-output-display execution_count=355}\n```\n1949-01         NaN\n1949-02    4.718760\n1949-03    4.770946\n1949-04    4.883063\n1949-05    4.860073\n             ...   \n1959-08    6.310106\n1959-09    6.138659\n1959-10    6.004945\n1959-11    5.869054\n1959-12    5.974289\nFreq: M, Length: 132, dtype: float64\n```\n:::\n:::\n\n\n## Obtain residual series\n\n::: {.cell execution_count=38}\n``` {.python .cell-code}\nresidual = y_train.naturallog - y_pred_train\nresidual\n```\n\n::: {.cell-output .cell-output-display execution_count=356}\n```\n1949-01         NaN\n1949-02    0.051925\n1949-03    0.111856\n1949-04   -0.023251\n1949-05   -0.064283\n             ...   \n1959-08    0.016043\n1959-09   -0.000931\n1959-10    0.003868\n1959-11    0.022590\n1959-12    0.029598\nFreq: M, Length: 132, dtype: float64\n```\n:::\n:::\n\n\n## Plot residuals\n\n::: {.cell execution_count=39}\n``` {.python .cell-code}\nplot_series(residual)\n```\n\n::: {.cell-output .cell-output-display execution_count=357}\n```\n(<Figure size 1920x480 with 1 Axes>, <AxesSubplot: >)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](Week3_files/figure-revealjs/cell-40-output-2.png){}\n:::\n:::\n\n\n## Plot residuals (cont.)\n\n::: {.cell execution_count=40}\n``` {.python .cell-code}\nplot_acf(residual.dropna(), lags=50)\n```\n\n::: {.cell-output .cell-output-display execution_count=358}\n![](Week3_files/figure-revealjs/cell-41-output-1.png){}\n:::\n\n::: {.cell-output .cell-output-display}\n![](Week3_files/figure-revealjs/cell-41-output-2.png){}\n:::\n:::\n\n\n## Plot residuals (cont.)\n\n::: {.cell execution_count=41}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport numpy as np\nplt.hist(residual)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](Week3_files/figure-revealjs/cell-42-output-1.png){}\n:::\n:::\n\n\nYour turn: remove the outlier and draw the histogram\n\n## Ljung-Box Test\n\nH0: Residuals are not serially correlated.\n\nH1: Residuals are serially correlated.\n\n::: {.cell execution_count=42}\n\n::: {.cell-output .cell-output-display execution_count=360}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lb_stat</th>\n      <th>lb_pvalue</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>20</th>\n      <td>3.984776</td>\n      <td>0.999955</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Step 7: Generate forecasts\n\n::: {.cell execution_count=43}\n``` {.python .cell-code}\ny_pred_1 = forecaster1.predict(fh)\ny_pred_1\n```\n\n::: {.cell-output .cell-output-display execution_count=361}\n```\n1960-01    6.037478\n1960-02    5.982107\n1960-03    6.133707\n1960-04    6.102795\n1960-05    6.154218\n1960-06    6.301004\n1960-07    6.437653\n1960-08    6.461717\n1960-09    6.257650\n1960-10    6.134112\n1960-11    6.003672\n1960-12    6.103036\nFreq: M, dtype: float64\n```\n:::\n:::\n\n\n## Back transformation\n\n::: {.cell execution_count=44}\n``` {.python .cell-code}\ny_pred_1.exp = np.exp(y_pred_1)\ny_pred_1.exp\n```\n\n::: {.cell-output .cell-output-display execution_count=362}\n```\n1960-01    418.835520\n1960-02    396.274338\n1960-03    461.142677\n1960-04    447.105543\n1960-05    470.698677\n1960-06    545.118980\n1960-07    624.938414\n1960-08    640.159214\n1960-09    521.990600\n1960-10    461.329316\n1960-11    404.912933\n1960-12    447.213408\nFreq: M, dtype: float64\n```\n:::\n:::\n\n\n## Plot training, test, and forecasts\n\n::: {.cell execution_count=45}\n``` {.python .cell-code}\nplot_series(y_train, y_test, y_pred_1.exp, labels=[\"y_train\", \"y_test\", \"y_forecast\"])\n```\n\n::: {.cell-output .cell-output-display execution_count=363}\n```\n(<Figure size 1920x480 with 1 Axes>,\n <AxesSubplot: ylabel='Number of airline passengers'>)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](Week3_files/figure-revealjs/cell-46-output-2.png){}\n:::\n:::\n\n\n## Evaluation\n\n::: {.cell execution_count=46}\n``` {.python .cell-code}\nfrom sktime.performance_metrics.forecasting import \\\n    mean_absolute_percentage_error\nmean_absolute_percentage_error(y_test, y_pred_1.exp, symmetric=False)\n```\n\n::: {.cell-output .cell-output-display execution_count=364}\n```\n0.027756916452706674\n```\n:::\n:::\n\n\n## Your Turn\n\nFit other variants of ARIMA models and identify the best ARIMA model for the series.\n\n## Modelling steps\n\n1. Plot the data.\n\n2. If necessary, transform the data (using a Box-Cox transformation) to stabilise the variance.\n\n3. If the data are non-stationary, take first differences of the data until the data are stationary.\n\n4. Examine the ACF/PACF to identify a suitable model.\n\n5. Try your chosen model(s), and use the AICc to search for a better model.\n\n## Modelling steps (cont.)\n\n6. Check the residuals from your chosen model by plotting the ACF of the residuals, and doing a portmanteau test of the residuals. If they do not look like white noise, try a modified model.\n\n7. Once the residuals look like white noise, calculate forecasts.\n\n\n\nSource: Forecasting: Principles and Practice, Rob J Hyndman and George Athanasopoulos\n\n",
    "supporting": [
      "Week3_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}